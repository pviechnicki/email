{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well can DOTCE learn to classify your emails?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: put your cursor in each code snippet box and press shift-enter to execute. Pay attention to the output of each step, checking for errors.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Go to correct directory and activate the DOTCE virtual env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pviechnicki\\Desktop\\pviechnicki_home\\sandbox\\state\\dotce\n"
     ]
    }
   ],
   "source": [
    "#Replace the directory string in line 3\n",
    "#with your own directory path where you stored your email files.\n",
    "%cd c:\\Users\\pviechnicki\\Desktop\\pviechnicki_home\\sandbox\\state\\dotce\n",
    "%pwd\n",
    "! activate dotce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load various machine learning libraries to extract features from text document corpus and build classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pviechnicki\\AppData\\Local\\Continuum\\Anaconda3\\envs\\dotce\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\pviechnicki\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda3\\\\envs\\\\dotce\\\\lib\\\\site-packages\\\\pyLDAvis')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Read csv file of emails into df and split into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: make sure you've exported 5-7 email folders from outlook as CSV files, then edit stack_email_files.py to reflect the names of the folders you've exported, then execute stack_email_files.py to create combined_emails.csv before running step 3. The path of your combined emails file needs to match what is in line 3 of step 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe contains 4629 messages\n",
      "Non-empty datafram contains 925 messages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Make sure we're in the right directory\n",
    "os.getcwd()\n",
    "email_df = pd.read_csv('C:\\\\Users\\\\pviechnicki\\\\Desktop\\\\pviechnicki_home\\\\sandbox\\\\state\\\\data\\\\pv_email\\\\combined_emails.csv', sep='|')\n",
    "#Add rowid\n",
    "email_df['rownum'] = range(0, len(email_df))\n",
    "email_df.groupby('cat').count()\n",
    "# Filter out empty rows\n",
    "non_empty_df = email_df[email_df['body'].isnull() == False].sample(frac=.2)\n",
    "#sample method chooses a random sample of the origina frame\n",
    "#https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "print(\"Original dataframe contains {} messages\\nNon-empty datafram contains {} messages\\n\".format(\n",
    "len(email_df), len(non_empty_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: change 'about_fraud' and 'fraud_waste_abuse' in the code snippet below to match the folders and categories you've chosen and exported.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    657\n",
       "True     268\n",
       "Name: about_fraud, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add new column to dataframe with True if cat == fraud_waste_abuse\n",
    "#Edit the column names and truth conditions to match your data\n",
    "non_empty_df['about_fraud'] = (non_empty_df['cat'] == 'fraud_waste_abuse')\n",
    "#Create a vector of class labels\n",
    "class_labels = non_empty_df['about_fraud']\n",
    "#use value_counts() method of series\n",
    "class_labels.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a training set and test set, 80% 20%\n",
    "train_df, test_df = train_test_split(non_empty_df, train_size = 0.8, random_state=44)\n",
    "class_labels_training = list(train_df['about_fraud'])\n",
    "class_labels_test = list(test_df['about_fraud'])\n",
    "value_counts = nltk.FreqDist(class_labels_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Let's make sure we can tokenize these properly and remove stop words\n",
    "<p>from <a href=\"http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\">CS Duke.edu</a></p>\n",
    "<p>Tip: Make sure you've installed the english punctuation and stop words list \n",
    "following <a href=\"http://www.nltk.org/data.html\">these instructions.</a> If you've done it right, you'll see a list of ten common words from your first email when you run step 4.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('us', 8), ('mahesh', 5), ('studi', 5), ('kelkar', 4), ('govern', 4), ('thank', 3), ('let', 3), ('know', 3), ('manag', 3), ('sector', 3)]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate a stemmer and a tokenizer to preprocess the email text\n",
    "\n",
    "snowballStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def preprocess(text):\n",
    "    no_punctuation_text = ''\n",
    "    if (type(text)== str):\n",
    "        lower_text = text.lower()\n",
    "        no_punctuation_text = lower_text.translate({ord(c):'' for c in string.punctuation})\n",
    "    return no_punctuation_text\n",
    "\n",
    "def myTokenize(text):\n",
    "    global snowballStemmer\n",
    "    tokens = []\n",
    "    cleaned = preprocess(text)\n",
    "    tokens = nltk.word_tokenize(cleaned)\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    stemmed = [w for w in map(snowballStemmer.stem, filtered)]\n",
    "    return stemmed\n",
    "    \n",
    "tokens = myTokenize(train_df['body'].iloc[0])\n",
    "count = Counter(tokens)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: convert train and test dfs to term X document representation matrices (_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: make sure the circle at top right labeled with the name of your kernerl ('dotce' in my case) turns white, showing that each step has completed, before you move on to the next step.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Instantiate a TFidf vectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, encoding='utf-8', \n",
    "                             max_df=0.5, tokenizer=myTokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This step can take a while\n",
    "train_X = vectorizer.fit_transform(train_df['body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Save feature names in a separate list\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/\n",
    "#Create another matrix of tfidf scores for the documents in the test set\n",
    "test_X = vectorizer.transform(test_df['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Instantiate and Train a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.05, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The alpha value is the sensitivity parameter.\n",
    "#We train the classifier by feeding it with the labeled training data we created in step 3 above.\n",
    "model_nb = MultinomialNB(alpha=0.05)\n",
    "model_nb.fit(train_X, class_labels_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test classifier on test_X matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NB Classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for your classifier: 0.973\n",
      "\n",
      "Error rate for your classifier: 0.027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_nb.predict( test_X )\n",
    "print(\"Accuracy score for your classifier: {:.3f}\\n\".format(model_nb.score( test_X, class_labels_test)))\n",
    "print(\"Error rate for your classifier: {:.3f}\\n\".format(1-model_nb.score( test_X, class_labels_test)))\n",
    "classifierStats = dict()\n",
    "classifierStats['accuracy'] = model_nb.score( test_X, class_labels_test)\n",
    "classifierStats['errorRate'] = (1 - model_nb.score( test_X, class_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truth_value(myRow):\n",
    "    if (myRow['ground_truth'] == True and myRow['predicted_value'] == True):\n",
    "        return 'truePositive'\n",
    "    elif (myRow['ground_truth'] == True and myRow['predicted_value'] == False):\n",
    "        return 'falseNegative'\n",
    "    elif (myRow['ground_truth'] == False and myRow['predicted_value'] == True):\n",
    "        return 'falsePositive'\n",
    "    elif (myRow['ground_truth'] == False and myRow['predicted_value'] == False):\n",
    "        return 'trueNegative'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "results = [(class_labels_test[i], predictions[i]) for i in range(0,len(predictions))]\n",
    "#Add in the email id, subject, and body, then truePos, falsePos, trueNeg, falseNeg\n",
    "enrichedResults = pd.DataFrame.from_records(results, test_df['rownum'].tolist(), \n",
    "    columns = ['ground_truth', 'predicted_value'])\n",
    "enrichedResults['truthValue'] = enrichedResults.apply(lambda row: truth_value(row), axis=1)\n",
    "enrichedResults['subject'] = test_df['subject'].tolist()\n",
    "enrichedResults['body'] = test_df['body'].tolist()\n",
    "counts = enrichedResults['truthValue'].value_counts()\n",
    "for i in range(0,len(counts)):\n",
    "    classifierStats[counts.index[i]] = counts[i]\n",
    "with open('classifierStats.pyc', 'wb') as f:\n",
    "    pickle.dump(classifierStats, f)\n",
    "f.close()\n",
    "with open('classifierTestResults.pyc', 'wb') as f1:\n",
    "    pickle.dump(enrichedResults, f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Results\n",
    "<p>Nice summary of different formulas for accuracy, precision, recall, etc \n",
    "<a href=\"http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf\">here</a>.</p>\n",
    "We're using the sklearn metrics module to evaluate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.98       140\n",
      "       True       0.93      0.96      0.95        45\n",
      "\n",
      "avg / total       0.97      0.97      0.97       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(class_labels_test, predictions))\n",
    "#Need to write this out to persistent object for use by visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Let's look more closely at how the classifier uses tf-idf scores to categorize emails\n",
    "<p>Tip of the hat to this blog: <a href=\"https://buhrmann.github.io/tfidf-analysis.html\">blog post from Thomas Buhrmann</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Returns top n tfidf features as df, but takes dense format vector as input\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "#convert single row into dense format\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>panzarella</td>\n",
       "      <td>0.274297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>edward</td>\n",
       "      <td>0.223563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ill</td>\n",
       "      <td>0.184952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urbanczyk</td>\n",
       "      <td>0.180370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rosslyn</td>\n",
       "      <td>0.174550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature     tfidf\n",
       "0  panzarella  0.274297\n",
       "1      edward  0.223563\n",
       "2         ill  0.184952\n",
       "3   urbanczyk  0.180370\n",
       "4     rosslyn  0.174550"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is showing us the top ten tfidf scores for document 3\n",
    "#This is a sanity check and should show you some informative words from within an email\n",
    "top_feats_in_doc(train_X, feature_names, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper function to calculate top n features that are on average most important among\n",
    "#documents with grp_ids = ?\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: change 'about_fraud' in line 5 below to match the category you selected in line 3 of step 3 above.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    ids = np.where(y.about_fraud==True)\n",
    "    feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "    feats_df.label = 'fraud_waste_abuse'\n",
    "    return feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "informativeTerms = top_feats_by_class(train_X, train_df, feature_names, top_n=100)\n",
    "#Should print out 10 most informative features for you\n",
    "informativeTerms.head(10)\n",
    "with open('informativeTerms.pyc', 'wb') as f:\n",
    "    pickle.dump(informativeTerms, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 9: write out most informative features and counts for training set and test set to power dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count up occurrences of the top 100 most informative features\n",
    "from collections import defaultdict\n",
    "feature_counts_training = defaultdict(int)\n",
    "feature_counts_test = defaultdict(int)\n",
    "informativeTermsSet = set(informativeTerms['feature'])\n",
    "trainTokensTotal = 0\n",
    "testTokensTotal = 0\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    for token in myTokenize(row['body']):\n",
    "        trainTokensTotal += 1\n",
    "        if (token in informativeTermsSet):\n",
    "            feature_counts_training[token] += 1 \n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    for token in myTokenize(row['body']):\n",
    "        testTokensTotal+= 1\n",
    "        if (token in informativeTermsSet):\n",
    "            feature_counts_test[token] += 1 \n",
    "\n",
    "#Write to disk\n",
    "with open('feature_counts_training.pyc', 'wb') as f:\n",
    "    pickle.dump(feature_counts_training, f)\n",
    "f.close()\n",
    "\n",
    "with open('feature_counts_training.pyc', 'wb') as f:\n",
    "    pickle.dump(feature_counts_training, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'junko', 's√©an', 'fight', 'william', 'iphon', 'sean', 'boston', 'pulkit', '844', 'snap', 'daniel', 'advisori', 'websit', 'eggersdirector', 'pukapoordeloittecomsubject', 'check', 'addit', 'program', 'file', 'decemb', '25', '5718826585fax', 'brien', 'researchdeloitt', 'book', 'lp', 'outlin', '2015', 'articl', 'consult', 'ok', 'thankspulkit', 'kaji', 'payment', 'hartford', 'gov2020', 'denver', 'thai', 'improp', 'fraud', 'steven', 'mailtostthaideloittecom', 'slide', 'final', 'april', 'httpwwwsolutionrevolutionbookcom', 'jake', 'error', 'wast', 'comment', 'revis', 'dan', 'novemb', 'tel', 'murphi', 'foglift', 'draft', 'wwwwilliameggerscom', 'tiffani', '12022469684', 'section', 'kapoor', 'wfe', 'tbishopdeloittecom', 'lorenz', 'japunzenbergerdeloittecom', 'httpgovernment2020dupresscom', 'graphic', 'februari', 'danolsondeloittecom', 'bishop', 'hood', 'new', 'crystal', 'miss', '7033981771mobil', 'abus', 'fwa', 'stthaideloittecom', 'kaleigh', 'edit', 'read', 'text', 'minneapoli', 'mailtoweggersdeloittecom', 'mailtotbishopdeloittecom', 'interact', 'benzadok', 'shooddeloittecom', 'studi', 'spreadsheet', 'mailtojapunzenbergerdeloittecom', 'acknowledg', 'framework', 'version', 'strategi', 'review', 'olson', 'punzenberg', 'troy'}\n"
     ]
    }
   ],
   "source": [
    "print(informativeTermsSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalTermCounts = {'trainTokensTotal': trainTokensTotal, 'testTokensTotal': testTokensTotal}\n",
    "with open('totalTermCounts.pyc', 'wb') as f:\n",
    "    pickle.dump(totalTermCounts, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotce",
   "language": "python",
   "name": "dotce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
