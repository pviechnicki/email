{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test partial_fit method of MultinomialNB and SGDClassifier models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: put your cursor in each code snippet box and press shift-enter to execute. Pay attention to the output of each step, checking for errors.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Go to correct directory and activate the DOTCE virtual env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pviechnicki\\Desktop\\pviechnicki_home\\sandbox\\state\\classify\n"
     ]
    }
   ],
   "source": [
    "#Replace the directory string in line 3\n",
    "#with your own directory path where you stored your email files.\n",
    "%cd c:\\Users\\pviechnicki\\Desktop\\pviechnicki_home\\sandbox\\state\\classify\n",
    "%pwd\n",
    "! activate dotce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load various machine learning libraries to extract features from text document corpus and build classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pickle\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from collections import defaultdict\n",
    "import operator #Used to sort dictionaries\n",
    "from sklearn import metrics\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Read csv file of emails into df and split into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: make sure you've exported 5-7 email folders from outlook as CSV files, then edit stack_email_files.py to reflect the names of the folders you've exported, then execute stack_email_files.py to create combined_emails.csv before running step 3. The path of your combined emails file needs to match what is in line 3 of step 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe contains 4629 messages\n",
      "Non-empty datafram contains 4164 messages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Make sure we're in the right directory\n",
    "os.getcwd()\n",
    "email_df = pd.read_csv('C:\\\\Users\\\\pviechnicki\\\\Desktop\\\\pviechnicki_home\\\\sandbox\\\\state\\\\data\\\\pv_email\\\\combined_emails.csv', sep='|')\n",
    "#Add rowid\n",
    "email_df['rownum'] = range(0, len(email_df))\n",
    "email_df.groupby('cat').count()\n",
    "# Filter out empty rows\n",
    "non_empty_df = email_df[email_df['body'].isnull() == False].sample(frac=.9)\n",
    "#sample method chooses a random sample of the origina frame\n",
    "#https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "print(\"Original dataframe contains {} messages\\nNon-empty datafram contains {} messages\\n\".format(\n",
    "len(email_df), len(non_empty_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3A: More data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need a function to split thread into messages\n",
    "def topMessage(text):\n",
    "    '''\n",
    "    return first in list of messages, splitting on delimiter\n",
    "    '''\n",
    "    messageDelimiter = ' From:'\n",
    "    return text.split(messageDelimiter)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: change 'about_fraud' and 'fraud_waste_abuse' in the code snippet below to match the folders and categories you've chosen and exported.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add new column to dataframe with True if cat == fraud_waste_abuse\n",
    "#Edit the column names and truth conditions to match your data\n",
    "non_empty_df['about_fraud'] = (non_empty_df['cat'] == 'fraud_waste_abuse')\n",
    "#Create a vector of class labels\n",
    "class_labels = non_empty_df['about_fraud']\n",
    "#use value_counts() method of series\n",
    "class_labels.value_counts()\n",
    "#Create derived column with subject + body\n",
    "non_empty_df['topMessage'] = non_empty_df['body'].apply(topMessage)\n",
    "non_empty_df['wholeMessage'] = (non_empty_df['subject'] + ' ' + non_empty_df['topMessage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a training set and test set, 80% 20%\n",
    "train_df, test_df = train_test_split(non_empty_df, train_size = 0.8, random_state=44)\n",
    "class_labels_training = list(train_df['about_fraud'])\n",
    "class_labels_test = list(test_df['about_fraud'])\n",
    "value_counts = nltk.FreqDist(class_labels_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add domain-specific words to stop-word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email_stopwords = ['http', 'bishop', 'eggers', 'pulkit', 'kapoor', 'dupress', \n",
    "                   'deloitte', 'mumbai', 'peter', 'viechnicki', 'arlington', 'www', 'com',\n",
    "                  'mkelkar', 'wegger', 'pviechnicki', 'mailto', 'sent', 'subject',\n",
    "                  'mahesh', 'https', 'troy', 'tbishop', 'jake', 'punzenburger',\n",
    "                  'pukapoor', 'brien', 'lorenz', 'laura', 'japunzenburg', 'audre', 'blorenze',\n",
    "                  'stthai', 'migreen', 'danolson', 'jguszcza']\n",
    "email_stopwords += stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Let's make sure we can tokenize these properly and remove stop words\n",
    "<p>from <a href=\"http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\">CS Duke.edu</a></p>\n",
    "<p>Tip: Make sure you've installed the english punctuation and stop words list \n",
    "following <a href=\"http://www.nltk.org/data.html\">these instructions.</a> If you've done it right, you'll see a list of ten common words from your first email when you run step 5.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('us', 20), ('tel', 17), ('join', 16), ('317728485', 15), ('phone', 15), ('cell', 14), ('seattl', 12), ('1', 11), ('joani', 8), ('davthoma', 8)]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate a stemmer and a tokenizer to preprocess the email text\n",
    "\n",
    "snowballStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def preprocess(text):\n",
    "    no_punctuation_text = ''\n",
    "    if (type(text)== str):\n",
    "        lower_text = text.lower()\n",
    "        no_punctuation_text = lower_text.translate({ord(c):' ' for c in string.punctuation})\n",
    "    return no_punctuation_text\n",
    "\n",
    "def myTokenize(text):\n",
    "    global snowballStemmer\n",
    "    tokens = []\n",
    "    cleaned = preprocess(text)\n",
    "    tokens = nltk.word_tokenize(cleaned)\n",
    "    filtered = [w for w in tokens if not w in email_stopwords]\n",
    "    stemmed = [w for w in map(snowballStemmer.stem, filtered)]\n",
    "    return stemmed\n",
    "    \n",
    "tokens = myTokenize(train_df['body'].iloc[0])\n",
    "count = Counter(tokens)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Select 500 meaningful terms as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need function to calculate chisq for each term\n",
    "#Then choose terms with highest chisq\n",
    "def chisq(myTerm, myCondition, myFreqDist, myConditionalFreqDist, N_cat, N):\n",
    "    N_emails_in_my_condition = N_cat\n",
    "    N_emails = N\n",
    "    observed = myConditionalFreqDist[myCondition][myTerm]/N_emails_in_my_condition\n",
    "    expected = myFreqDist[myTerm]/N_emails\n",
    "    chisq = (observed - expected)**2/expected\n",
    "    return chisq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist()\n",
    "cfdist = ConditionalFreqDist()\n",
    "vocabulary = set()\n",
    "termChisqTableTrue = defaultdict(float)\n",
    "termChisqTableFalse = defaultdict(float)\n",
    "\n",
    "N_cat = len(train_df.loc[train_df['about_fraud'] == True])\n",
    "N = len(train_df)\n",
    "for index, row in train_df.iterrows():\n",
    "    subjectTokens = myTokenize(row['subject'])\n",
    "    bodyTokens = myTokenize(topMessage(row['body']))\n",
    "    condition = row['about_fraud']\n",
    "    \n",
    "    #Store them in regular and conditional frequency distributions\n",
    "    for token in (subjectTokens + bodyTokens):\n",
    "        fdist[token] += 1\n",
    "        cfdist[condition][token] += 1\n",
    "        vocabulary.add(token)\n",
    "\n",
    "for term in vocabulary:\n",
    "    termChisqTableTrue[term] = chisq(term, True, fdist, cfdist, N_cat, N)\n",
    "    termChisqTableFalse[term] = chisq(term, False, fdist, cfdist, (N-N_cat), N)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select best 5000 features from each condition\n",
    "bestTermsAndScores = sorted(termChisqTableTrue.items(), key=operator.itemgetter(1), reverse=True)[:100]\n",
    "bestTerms = [term for (term, chisq) in bestTermsAndScores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: convert train and test dfs to term X document representation matrices (_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Tip: make sure the circle at top right labeled with the name of your kernerl ('dotce' in my case) turns white, showing that each step has completed, before you move on to the next step.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Instantiate a TFidf vectorizer and count vectorizer\n",
    "tv = TfidfVectorizer(encoding='utf-8', tokenizer=myTokenize, vocabulary = bestTerms)\n",
    "cv = CountVectorizer(encoding='utf-8',tokenizer=myTokenize, vocabulary=bestTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need a function to generate minibatches of text records and categories from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(myDf, startPosition, batch_size = 100):\n",
    "    dfSlice = myDf[startPosition: (startPosition+batch_size)]\n",
    "    X_text = dfSlice['wholeMessage'].astype('U')\n",
    "    y_cats = list(dfSlice['about_fraud'])\n",
    "    return (X_text, y_cats)\n",
    "\n",
    "def generate_minibatches(myDf, size = 100):\n",
    "    startPos = 0\n",
    "    X_records, y_cats = get_minibatch(myDf, startPos, batch_size = size)\n",
    "    while len(X_records):\n",
    "        startPos = startPos + size\n",
    "        yield X_records, y_cats\n",
    "        X_records, y_cats = get_minibatch(myDf, startPos, batch_size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test to make sure generator is working...\n",
    "junk = next(generate_minibatches(train_df, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/\n",
    "#Create another matrix of vectors for the documents in the test set\n",
    "test_X = cv.transform(test_df['wholeMessage'].astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Instantiate and Train a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The alpha value is the sensitivity parameter.\n",
    "#We train the classifier by feeding it with the labeled training data we created in step 3 above.\n",
    "model_nb = MultinomialNB(alpha=1)\n",
    "\n",
    "n_samples = len(train_df)\n",
    "c = .5\n",
    "myAlpha = 1/(c * n_samples)\n",
    "model_svm = SGDClassifier(loss='hinge', alpha = myAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental training on batch 0\n",
      "\n",
      "Incremental training on batch 1\n",
      "\n",
      "Incremental training on batch 2\n",
      "\n",
      "Incremental training on batch 3\n",
      "\n",
      "Incremental training on batch 4\n",
      "\n",
      "Incremental training on batch 5\n",
      "\n",
      "Incremental training on batch 6\n",
      "\n",
      "Incremental training on batch 7\n",
      "\n",
      "Incremental training on batch 8\n",
      "\n",
      "Incremental training on batch 9\n",
      "\n",
      "Incremental training on batch 10\n",
      "\n",
      "Incremental training on batch 11\n",
      "\n",
      "Incremental training on batch 12\n",
      "\n",
      "Incremental training on batch 13\n",
      "\n",
      "Incremental training on batch 14\n",
      "\n",
      "Incremental training on batch 15\n",
      "\n",
      "Incremental training on batch 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (train_docs_subset, class_labels_subset) in enumerate(generate_minibatches(train_df, 200)):\n",
    "    X_train = vectorizer.transform(train_docs_subset)\n",
    "    print(\"Incremental training on batch {}\\n\".format(i))\n",
    "    model_nb.partial_fit(X_train, class_labels_subset, classes = [False, True])\n",
    "    model_svm.partial_fit(X_train, class_labels_subset, classes = [False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test classifier on test_X matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NB Classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for your Naive Bayes classifier: 0.938\n",
      "\n",
      "Error rate for your Naive Bayes classifier: 0.062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#First get the predicted class label for each document\n",
    "predictions_nb = model_nb.predict( test_X )\n",
    "print(\"Accuracy score for your Naive Bayes classifier: {:.3f}\\n\".format(model_nb.score( test_X, class_labels_test)))\n",
    "print(\"Error rate for your Naive Bayes classifier: {:.3f}\\n\".format(1-model_nb.score( test_X, class_labels_test)))\n",
    "classifierStats = dict()\n",
    "classifierStats['accuracy'] = model_nb.score( test_X, class_labels_test)\n",
    "classifierStats['errorRate'] = (1 - model_nb.score( test_X, class_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for your SVM classifier: 0.896\n",
      "\n",
      "Error rate for your SVM classifier: 0.104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_svm = model_svm.predict( test_X )\n",
    "print(\"Accuracy score for your SVM classifier: {:.3f}\\n\".format(model_svm.score( test_X, class_labels_test)))\n",
    "print(\"Error rate for your SVM classifier: {:.3f}\\n\".format(1-model_svm.score( test_X, class_labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.99981150e-01,   1.88498898e-05])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Also store the predicted class probabilities\n",
    "predictProbabilities = model_nb.predict_proba( test_X )\n",
    "predictProbabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truth_value(myRow):\n",
    "    if (myRow['ground_truth'] == True and myRow['predicted_value'] == True):\n",
    "        return 'truePositive'\n",
    "    elif (myRow['ground_truth'] == True and myRow['predicted_value'] == False):\n",
    "        return 'falseNegative'\n",
    "    elif (myRow['ground_truth'] == False and myRow['predicted_value'] == True):\n",
    "        return 'falsePositive'\n",
    "    elif (myRow['ground_truth'] == False and myRow['predicted_value'] == False):\n",
    "        return 'trueNegative'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "results = [(class_labels_test[i], predictions[i]) for i in range(0,len(predictions))]\n",
    "#Add in the email id, subject, and body, then truePos, falsePos, trueNeg, falseNeg, posProbability, negProbability\n",
    "enrichedResults = pd.DataFrame.from_records(results, test_df['rownum'].tolist(), \n",
    "    columns = ['ground_truth', 'predicted_value'])\n",
    "enrichedResults['truthValue'] = enrichedResults.apply(lambda row: truth_value(row), axis=1)\n",
    "enrichedResults['subject'] = test_df['subject'].tolist()\n",
    "enrichedResults['body'] = test_df['body'].tolist()\n",
    "enrichedResults['posProbability'] = [prob[1] for prob in predictProbabilities]\n",
    "enrichedResults['negProbability'] = [prob[0] for prob in predictProbabilities]\n",
    "counts = enrichedResults['truthValue'].value_counts()\n",
    "for i in range(0,len(counts)):\n",
    "    classifierStats[counts.index[i]] = counts[i]\n",
    "with open('classifierStats.pyc', 'wb') as f:\n",
    "    pickle.dump(classifierStats, f)\n",
    "f.close()\n",
    "with open('classifierTestResults.pyc', 'wb') as f1:\n",
    "    pickle.dump(enrichedResults, f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True], dtype=bool)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Results\n",
    "<p>Nice summary of different formulas for accuracy, precision, recall, etc \n",
    "<a href=\"http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf\">here</a>.</p>\n",
    "We're using the sklearn metrics module to evaluate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.94      0.98      0.96       589\n",
      "       True       0.94      0.84      0.89       244\n",
      "\n",
      "avg / total       0.94      0.94      0.94       833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(class_labels_test, predictions))\n",
    "#Need to write this out to persistent object for use by visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 11: write out most informative features and counts for training set and test set to power dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.9017\tus\n",
      "\t-2.5453\tfraud\n",
      "\t-2.8010\t1\n",
      "\t-2.9733\twast\n",
      "\t-3.2754\tprogram\n",
      "\t-3.6246\terror\n",
      "\t-3.7652\tgovern\n",
      "\t-3.7939\tinteract\n",
      "\t-3.8678\tabus\n",
      "\t-3.9326\ttel\n"
     ]
    }
   ],
   "source": [
    "def mostInformativeFeatures(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = coefs_with_fns[:-(n + 1):-1]\n",
    "    return(top)\n",
    "    \n",
    "        \n",
    "bestFeatures = mostInformativeFeatures(vectorizer, model_nb, 100)\n",
    "\n",
    "for (coef, fn) in bestFeatures[0:10]:\n",
    "        print (\"\\t{:.4f}\\t{}\".format(coef, fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraud|-2.5453|1.7731|0.3117\n",
      "wast|-2.9733|1.1542|0.2029\n",
      "abus|-3.8678|0.4671|0.0821\n",
      "error|-3.6246|0.4423|0.0778\n",
      "program|-3.2754|0.4365|0.0767\n",
      "ai|-8.8341|0.4042|0.0710\n",
      "payment|-4.0805|0.3697|0.0650\n",
      "ffctn|-4.1520|0.3672|0.0645\n",
      "fwa|-4.2594|0.3296|0.0579\n",
      "improp|-4.3123|0.3128|0.0550\n"
     ]
    }
   ],
   "source": [
    "# Don't think I need this \n",
    "#Count up occurrences of the top 100 most informative features\n",
    "import csv\n",
    "\n",
    "def lookupCoef(myTerm, myBestFeatures):\n",
    "    for (coef, fn) in myBestFeatures:\n",
    "        if fn == myTerm:\n",
    "            return(coef)\n",
    "    return False\n",
    "\n",
    "with open(\"termScores.csv\", 'wt', encoding='utf8', newline='') as f:\n",
    "    \n",
    "    f_csv = csv.writer(f, delimiter=chr(31))\n",
    "    f_csv.writerow(['term', 'modelCoef', 'posChisq', 'negChisq'])\n",
    "    \n",
    "    termNo = 0\n",
    "    for term in bestTerms:\n",
    "        termNo += 1\n",
    "        modelCoef = lookupCoef(term, bestFeatures)\n",
    "        posChisq = termChisqTableTrue[term]\n",
    "        negChisq = termChisqTableFalse[term]\n",
    "\n",
    "        f_csv.writerow([term, modelCoef, posChisq, negChisq])\n",
    "        if termNo <= 10:\n",
    "            print(\"{}|{:.4f}|{:.4f}|{:.4f}\".format(term, modelCoef, posChisq, negChisq))\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotce",
   "language": "python",
   "name": "dotce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
